{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy as sp\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## questão 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.cluster.hierarchy import dendrogram,linkage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# a matriz dada no exercício precisou ser expandida (espelhada)\n",
    "# porque o python aparentemente precisa de entradas em todos os \n",
    "# elementos da matriz\n",
    "\n",
    "a = np.array([[0,2,6,10,9],\n",
    "              [2,0,5,9,8],\n",
    "              [6,5,0,4,5],\n",
    "              [10,9,4,0,3],\n",
    "              [9,8,5,3,0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAW0AAAD+CAYAAADxhFR7AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nMdqS1IjRlqRGjLYkNWK0JakRoy1Jjfw/6CEf5AJmj3MAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fd97998fa90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "z = linkage(a,method=\"single\")\n",
    "d=dendrogram(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As alternativas **b** e **e** estão corretas:\n",
    "\n",
    "- b) os objetos 1 e 2 formam o primeiro cluster não unitário\n",
    "\n",
    "- e) o algoritmo produz uma sequência de partições unitárias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## questão 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = np.array([[1,1],\n",
    "              [1,2],\n",
    "              [2,1],\n",
    "              [2,2],\n",
    "              [5,1],\n",
    "              [6,1],\n",
    "              [5,2]])\n",
    "\n",
    "initial_centroids = np.array([[3,0],\n",
    "                             [5,0]])\n",
    "\n",
    "cluster = KMeans(\n",
    "    n_clusters=2, # numero de clusters que eu quero\n",
    "    init=initial_centroids, # centroides iniciais\n",
    "    max_iter=5, # numero de iterações\n",
    "    n_init=1 # numero de vezes que o algo. vai rodar\n",
    ")\n",
    "\n",
    "cluster.fit(X)\n",
    "\n",
    "final_centroids = cluster.cluster_centers_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.5         1.5       ]\n",
      " [ 5.33333333  1.33333333]]\n"
     ]
    }
   ],
   "source": [
    "print(final_centroids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Os centróides finais obtidos são (1.5, 1.5) e (5.33, 5.33)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## questão 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics.cluster import adjusted_rand_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.166666666667\n"
     ]
    }
   ],
   "source": [
    "ground_truth = [1,2,1,2,2,1,2]\n",
    "pred = [1,2,1,1,1,2,2]\n",
    "\n",
    "print(adjusted_rand_score(ground_truth,pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## questão 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A opção **d** é verdadeira. Nenhuma das alternativas está correta."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## questão 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> estratégia: calcular $P(não|ensolarado, quente, normal, verdadeiro)$ e depois\n",
    "calcular $P(sim|ensolarado, quente, normal, verdadeiro)$ e ver \n",
    "qual dos dois é mais alto."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$P(não|ensolarado,quente,normal,verdadeiro)=\\frac{P(ensolarado, quente, normal, verdadeiro|não) * P(não)}{P(ensolarado, quente, normal, verdadeiro)}$\n",
    "\n",
    "              \n",
    " e $P(ensolarado, quente, normal, verdadeiro)$ \n",
    " é igual ao somatório de  $P(ensolarado, quente, normal, verdadeiro)$\n",
    " condicionado em \"não\" e condicionado em \"sim\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "asssumindo que as features são independentes entre si (hipótese ingênua), então $P(a,b) = P(a)P(b)$\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# P(ensolarado, quente, normal, verdadeiro| não)\n",
    "\n",
    "P_features_given_no_times_p_no = (3/5)* (5/14) + (3/5)*(5/14) + (1/5)*(5/14) + (3/5)*(5/14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# P(ensolarado, quente, normal, verdadeiro| sim)\n",
    "\n",
    "P_features_given_yes_times_p_yes = (2/9)*(9/15) + (2/9)*(9/15) + (6/9)*(9/15) + (3/9)*(9/15)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "denominator = P_features_given_no_times_p_no + P_features_given_yes_times_p_yes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.45180722891566266\n"
     ]
    }
   ],
   "source": [
    "# probabilidade de NÃO\n",
    "P_no_given_features = P_features_given_no_times_p_no / denominator\n",
    "print(P_no_given_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5481927710843373\n"
     ]
    }
   ],
   "source": [
    "# probabilidade de SIM\n",
    "P_yes_given_features = P_features_given_yes_times_p_yes / denominator\n",
    "print(P_yes_given_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A probabilidade $P(NÃO|features) = 0.452$ e $P(SIM|features)= 0.548$.\n",
    "\n",
    "Logo, a classe mais provável é **SIM**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## questão 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((12, 4), (12,))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# preciso converter as features categóricas em números\n",
    "\n",
    "# sexta/sábado: não/sim = 0,1\n",
    "# faminto: não/sim = 0,1 \n",
    "# clientes: cheio, nenhum, algum = 0,1,2\n",
    "# tipo: tailandes,frances, hamburguer, italiano = 0,1,2,3\n",
    "# espera: não/sim = 0,1\n",
    "\n",
    "encoded = np.array([\n",
    "    [0,1,0,0,0],\n",
    "    [1,0,0,1,0],\n",
    "    [0,0,1,2,0],\n",
    "    [1,0,1,2,0],\n",
    "    [1,1,0,3,0],\n",
    "    [0,0,1,0,0],\n",
    "    [0,1,2,1,1],\n",
    "    [0,0,2,2,1],\n",
    "    [1,1,0,0,1],\n",
    "    [0,1,2,3,1],\n",
    "    [0,1,2,0,1],\n",
    "    [1,1,0,2,1]\n",
    "])\n",
    "\n",
    "X = encoded[:,:-1]\n",
    "y = encoded[:,-1]\n",
    "X.shape,y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# critério default é o coeficiente gini\n",
    "clf = tree.DecisionTreeClassifier(criterion='entropy')\n",
    "clf = clf.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# item A\n",
    "# [1,1,0,1] = [sim, sim, cheio, francês]\n",
    "x_test = np.array([1,1,0,1]).reshape(1,-1)\n",
    "\n",
    "clf.predict(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a classe prevista é **1**, ou seja, espera=**SIM**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# item B\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# função de distância escolhida só funciona com features binárias,\n",
    "# ou seja, one hot encoded.\n",
    "enc = OneHotEncoder()\n",
    "encoded_ohe = enc.fit_transform(encoded[:,:4]).todense()\n",
    "\n",
    "# [1,1,1,3] = [sim,sim,alguns,italiano]\n",
    "x_test = np.array([1,1,2,3]).reshape(1,-1)\n",
    "\n",
    "test_ohe = enc.transform(x_test).todense()\n",
    "\n",
    "# using \"matching\" metric to match what's been described in the\n",
    "# test description\n",
    "clf = KNeighborsClassifier(n_neighbors=3,metric=\"matching\")\n",
    "\n",
    "clf.fit(encoded_ohe,encoded[:,-1])\n",
    "\n",
    "clf.predict(test_ohe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De acordo com o método kNN, a classe prevista para o caso de teste é **1**, ou seja, **espera=sim**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## questão 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## questão 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## questão 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## questão 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- (V)\n",
    "- (F) - Falso - o hdfs é o sistema de arquivos usado pelo hadoop\n",
    "- (V) - Verdadeiro, mas o dialeto de SQL usado (HiveQL) não segue 100% o padrão SQL\n",
    "- (V) - Verdadeiro, a menos que se use mais de um name node\n",
    "- (V) - Verdadeiro, se o Spark for inicializado com suporte ao Hive\n",
    "- (F) - Falso - há projetos que integram os dois, mas não de forma nativa\n",
    "- (F) - Falso - essa funcionalidade é feita justamente para que se possa usar scripts feitos em qualquer linguagem pelo Hadoop\n",
    "- (F) - Falso - o Hive tem um otimizador/planejador de consultas mas é possível otimizar a forma como as consultas são escritas, por exemplo usar GROUP BY primeiro por um atributo com cardinalidade alta, pois isso permite que o dado seja bem particionado, logo o job pode ser distribuído em vários nós.\n",
    "-  (V) Verdadeiro - se o resultado de um passo do seu processo no Spark resultar em uma única partição,  Spark só vai conseguir utilizar um nó para processar o próximo passo. Uma alternativa é forçar um shuffle para que o dado seja re-particionado em várias partes novamente.\n",
    "- (F) - Falso - o YARN também pode ser usado para gerenciar recursos de um cluster Hadoop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Global TF Kernel (Python 3)",
   "language": "python",
   "name": "global-tf-python-3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
